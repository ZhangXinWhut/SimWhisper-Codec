# WERè®¡ç®—è„šæœ¬ä½¿ç”¨è¯´æ˜

## åŠŸèƒ½æ¦‚è¿°

æ­¤è„šæœ¬ç”¨äºè®¡ç®—é‡å»ºéŸ³é¢‘çš„è¯é”™è¯¯ç‡ï¼ˆWord Error Rate, WERï¼‰ï¼Œé€šè¿‡å¯¹æ¯”HuBERTæ¨¡å‹è½¬å½•çš„é‡å»ºéŸ³é¢‘ä¸LibriSpeech test-cleanæ•°æ®é›†çš„å‚è€ƒè½¬å½•æ–‡æœ¬ã€‚

## æ ¸å¿ƒåŠŸèƒ½

1. **è‡ªåŠ¨éŸ³é¢‘è½¬å½•**ï¼šä½¿ç”¨HuBERTå¤§æ¨¡å‹è½¬å½•é‡å»ºçš„éŸ³é¢‘æ–‡ä»¶
2. **WERè®¡ç®—**ï¼šè®¡ç®—è½¬å½•æ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬çš„è¯é”™è¯¯ç‡
3. **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒå¤„ç†å¤§é‡éŸ³é¢‘æ–‡ä»¶
4. **ç»“æœä¿å­˜**ï¼šä¿å­˜è¯¦ç»†çš„è½¬å½•å¯¹æ¯”å’Œç»Ÿè®¡ç»“æœ

## ä¾èµ–è¦æ±‚

### PythonåŒ…ä¾èµ–
```bash
pip install jiwer transformers torch torchaudio tqdm numpy
```

### æ¨¡å‹ä¾èµ–
- HuBERT-Largeæ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åï¼‰
- é»˜è®¤ä½¿ç”¨ï¼š`facebook/hubert-large-ls960-ft`

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•
```bash
python scripts/calculate_wer.py \
    --eval-results-dir post_evaluation_results \
    --manifest-path data/manifests/librispeech/librispeech_test_clean.jsonl \
    --hubert-model-path /root/autodl-tmp/hubert_large_model \
    --device auto \
    --output-dir wer_results
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--eval-results-dir` | str | **å¿…éœ€** | è¯„ä¼°ç»“æœç›®å½•ï¼ˆåŒ…å«éŸ³é¢‘å’Œå…ƒæ•°æ®æ–‡ä»¶ï¼‰ |
| `--manifest-path` | str | `librispeech_test_clean.jsonl` | LibriSpeech test-clean manifestæ–‡ä»¶è·¯å¾„ |
| `--hubert-model-path` | str | `/root/autodl-tmp/hubert_large_model` | HuBERTæ¨¡å‹è·¯å¾„ |
| `--device` | str | `auto` | è®¡ç®—è®¾å¤‡ (auto, cpu, cuda, cuda:0ç­‰) |
| `--output-dir` | str | `wer_results` | ç»“æœè¾“å‡ºç›®å½• |
| `--max-samples` | int | `None` | æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨ |

### å®Œæ•´ç¤ºä¾‹

```bash
# å¤„ç†æ‰€æœ‰2620ä¸ªæ ·æœ¬
python scripts/calculate_wer.py \
    --eval-results-dir post_evaluation_results \
    --manifest-path data/manifests/librispeech/librispeech_test_clean.jsonl \
    --hubert-model-path /root/autodl-tmp/hubert_large_model \
    --device cuda:0 \
    --output-dir wer_full_results

# æµ‹è¯•å‰100ä¸ªæ ·æœ¬
python scripts/calculate_wer.py \
    --eval-results-dir post_evaluation_results \
    --manifest-path data/manifests/librispeech/librispeech_test_clean.jsonl \
    --hubert-model-path /root/autodl-tmp/hubert_large_model \
    --device auto \
    --output-dir wer_test_results \
    --max-samples 100

# åå°è¿è¡Œå®Œæ•´è®¡ç®—
nohup python scripts/calculate_wer.py \
    --eval-results-dir post_evaluation_results \
    --manifest-path data/manifests/librispeech/librispeech_test_clean.jsonl \
    --hubert-model-path /root/autodl-tmp/hubert_large_model \
    --device auto \
    --output-dir wer_full_results > wer_calculation.log 2>&1 &
```

## è¾“å‡ºç»“æœ

### æ§åˆ¶å°è¾“å‡º
```
ğŸ“Š WERè®¡ç®—ç»“æœ
============================================================
ğŸ¯ æ€»ä½“WER: 0.0102 (1.02%)
ğŸ“ˆ å¹³å‡ä¸ªä½“WER: 0.0065 (Â±0.0129)
ğŸ”º æœ€é«˜WER: 0.0323
ğŸ”» æœ€ä½WER: 0.0000
ğŸ“Š æˆåŠŸå¤„ç†æ ·æœ¬æ•°: 5/5
```

### æ–‡ä»¶è¾“å‡ºç»“æ„
```
wer_results/
â”œâ”€â”€ wer_results.json          # è¯¦ç»†ç»Ÿè®¡ç»“æœ
â”œâ”€â”€ transcriptions.jsonl      # é€æ¡è½¬å½•å¯¹æ¯”
â””â”€â”€ wer_calculation.log       # è¿è¡Œæ—¥å¿—
```

### ç»“æœæ–‡ä»¶å†…å®¹

#### `wer_results.json`
```json
{
  "overall_wer": 0.0102,
  "avg_individual_wer": 0.0065,
  "std_individual_wer": 0.0129,
  "min_wer": 0.0000,
  "max_wer": 0.0323,
  "num_samples": 5,
  "successful_transcriptions": 5,
  "total_metadata_files": 5,
  "individual_results": [...]
}
```

#### `transcriptions.jsonl`
æ¯è¡ŒåŒ…å«ä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼š
```json
{
  "sample_id": 0,
  "original_filename": "6930-75918-0000",
  "dataset_name": "librispeech_test_clean",
  "reference_text": "CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS",
  "hypothesis_text": "CONCORD RETURNED TO ITS PLACE AMIDST THE TENTS",
  "wer": 0.0,
  "pesq_wb": 2.1805808544158936,
  "pesq_nb": 2.962635040283203,
  "stoi_score": 0.9201543337841255
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ç¡¬ä»¶è¦æ±‚
- **GPUå†…å­˜**ï¼šå»ºè®®8GBä»¥ä¸ŠVRAMï¼ˆç”¨äºHuBERTæ¨¡å‹æ¨ç†ï¼‰
- **ç³»ç»Ÿå†…å­˜**ï¼šå»ºè®®16GBä»¥ä¸ŠRAM
- **å­˜å‚¨ç©ºé—´**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´å­˜å‚¨è½¬å½•ç»“æœ

### æ‰¹å¤„ç†ä¼˜åŒ–
- å¯¹äºå¤§é‡æ ·æœ¬ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†ï¼š
  ```bash
  # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ¬¡1000ä¸ªæ ·æœ¬
  python scripts/calculate_wer.py --max-samples 1000 --output-dir wer_batch1
  python scripts/calculate_wer.py --max-samples 1000 --output-dir wer_batch2 # éœ€è¦ä¿®æ”¹è„šæœ¬æ”¯æŒèµ·å§‹åç§»
  ```

### è®¾å¤‡é€‰æ‹©
- **GPUæ¨ç†**ï¼š`--device cuda:0`ï¼ˆæ¨èï¼Œé€Ÿåº¦å¿«ï¼‰
- **CPUæ¨ç†**ï¼š`--device cpu`ï¼ˆè¾ƒæ…¢ä½†å†…å­˜å ç”¨å°ï¼‰
- **è‡ªåŠ¨é€‰æ‹©**ï¼š`--device auto`ï¼ˆé»˜è®¤ï¼Œè‡ªåŠ¨æ£€æµ‹æœ€ä¼˜è®¾å¤‡ï¼‰

## ç»“æœè§£è¯»

### WERæŒ‡æ ‡è¯´æ˜
- **æ€»ä½“WER**ï¼šæ‰€æœ‰æ ·æœ¬è¿æ¥åè®¡ç®—çš„WERï¼ˆæ¨èæŒ‡æ ‡ï¼‰
- **å¹³å‡ä¸ªä½“WER**ï¼šæ¯ä¸ªæ ·æœ¬WERçš„å¹³å‡å€¼
- **æ ‡å‡†å·®**ï¼šWERåˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦
- **æœ€å€¼**ï¼šæœ€é«˜å’Œæœ€ä½WERåˆ†æ•°

### è´¨é‡è¯„ä¼°æ ‡å‡†
- **ä¼˜ç§€**ï¼šWER < 5%
- **è‰¯å¥½**ï¼šWER 5-15%
- **å¯æ¥å—**ï¼šWER 15-30%
- **è¾ƒå·®**ï¼šWER > 30%

### ä¸å…¶ä»–æŒ‡æ ‡çš„å…³ç³»
è„šæœ¬åŒæ—¶æŠ¥å‘Šï¼š
- **PESQåˆ†æ•°**ï¼šæ„ŸçŸ¥éŸ³é¢‘è´¨é‡è¯„ä¼°
- **STOIåˆ†æ•°**ï¼šçŸ­æ—¶é—´å®¢è§‚å¯æ‡‚åº¦æŒ‡æ•°

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   ```
   RuntimeError: CUDA out of memory
   ```
   è§£å†³ï¼šä½¿ç”¨ `--device cpu` æˆ–å‡å°‘ `--max-samples`

2. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```
   OSError: Can't load tokenizer
   ```
   è§£å†³ï¼šæ£€æŸ¥HuBERTæ¨¡å‹è·¯å¾„ï¼Œæˆ–ä½¿ç”¨HuggingFaceæ¨¡å‹å

3. **éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨**
   ```
   WARNING: é‡å»ºéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨
   ```
   è§£å†³ï¼šæ£€æŸ¥ `--eval-results-dir` è·¯å¾„æ˜¯å¦æ­£ç¡®

4. **è½¬å½•å¤±è´¥**
   ```
   WARNING: è½¬å½•å¤±è´¥
   ```
   è§£å†³ï¼šæ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼å’Œå®Œæ•´æ€§

### è°ƒè¯•æ¨¡å¼
å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```bash
python scripts/calculate_wer.py --max-samples 5  # å…ˆç”¨å°‘é‡æ ·æœ¬æµ‹è¯•
```

### æ€§èƒ½ç›‘æ§
```bash
# ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# ç›‘æ§CPUå’Œå†…å­˜ä½¿ç”¨
htop

# æŸ¥çœ‹åå°ä»»åŠ¡è¿›åº¦
tail -f wer_calculation.log
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰HuBERTæ¨¡å‹
ä½¿ç”¨ä¸åŒçš„HuBERTæ¨¡å‹ï¼š
```bash
# ä½¿ç”¨HuggingFaceæ¨¡å‹
python scripts/calculate_wer.py \
    --hubert-model-path facebook/hubert-large-ls960-ft \
    --eval-results-dir post_evaluation_results

# ä½¿ç”¨å…¶ä»–æœ¬åœ°æ¨¡å‹
python scripts/calculate_wer.py \
    --hubert-model-path /path/to/custom/hubert/model \
    --eval-results-dir post_evaluation_results
```

### ç»“æœåˆå¹¶
å¦‚æœåˆ†æ‰¹å¤„ç†ï¼Œå¯ä»¥æ‰‹åŠ¨åˆå¹¶ç»“æœï¼š
```python
import json

# åˆå¹¶å¤šä¸ªæ‰¹æ¬¡çš„ç»“æœ
results = []
for i in range(1, 4):  # 3ä¸ªæ‰¹æ¬¡
    with open(f'wer_batch{i}/transcriptions.jsonl', 'r') as f:
        for line in f:
            results.append(json.loads(line))

# è®¡ç®—æ€»ä½“WER
import jiwer
references = [r['reference_text'] for r in results]
hypotheses = [r['hypothesis_text'] for r in results]
overall_wer = jiwer.wer(' '.join(references), ' '.join(hypotheses))
print(f"åˆå¹¶åæ€»ä½“WER: {overall_wer:.4f}")
```

## æ³¨æ„äº‹é¡¹

1. **æ•°æ®ä¸€è‡´æ€§**ï¼šç¡®ä¿è¯„ä¼°ç»“æœç›®å½•ä¸­çš„éŸ³é¢‘ä¸manifestæ–‡ä»¶å¯¹åº”
2. **æ¨¡å‹ç‰ˆæœ¬**ï¼šä¸åŒç‰ˆæœ¬çš„HuBERTæ¨¡å‹å¯èƒ½äº§ç”Ÿä¸åŒçš„è½¬å½•ç»“æœ
3. **é‡‡æ ·ç‡**ï¼šè„šæœ¬è‡ªåŠ¨å¤„ç†é‡‡æ ·ç‡è½¬æ¢ï¼Œä½†å»ºè®®ä½¿ç”¨16kHzéŸ³é¢‘
4. **æ–‡æœ¬è§„èŒƒåŒ–**ï¼šè½¬å½•æ–‡æœ¬ä¼šè‡ªåŠ¨å¤§å†™åŒ–ï¼Œä¸LibriSpeechæ ¼å¼ä¿æŒä¸€è‡´

## æ‰©å±•åŠŸèƒ½

è¯¥è„šæœ¬å¯ä»¥è½»æ¾æ‰©å±•æ”¯æŒï¼š
- å…¶ä»–ASRæ¨¡å‹ï¼ˆWhisperã€Wav2Vec2ç­‰ï¼‰
- å…¶ä»–æ•°æ®é›†æ ¼å¼
- å¤šè¯­è¨€WERè®¡ç®—
- å­—ç¬¦çº§é”™è¯¯ç‡è®¡ç®—ï¼ˆCERï¼‰

å¦‚éœ€å®šåˆ¶åŒ–åŠŸèƒ½ï¼Œè¯·ä¿®æ”¹ `WERCalculator` ç±»çš„ç›¸åº”æ–¹æ³•ã€‚
